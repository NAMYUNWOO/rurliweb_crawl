{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__==\"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from scrapy import Selector\n",
    "import re\n",
    "#browser = webdriver.PhantomJS(\"C:\\\\Users\\\\Yunwoo\\\\Desktop\\\\webdriver\\\\phantomjs\\\\bin\\\\phantomjs.exe\")\n",
    "browser = webdriver.Chrome(\"C:\\\\Users\\\\Yunwoo\\\\Desktop\\\\webdriver\\\\chromedriver.exe\")\n",
    "\n",
    "def getCommentInfo_Page():\n",
    "    t1 = time.time()\n",
    "    time.sleep(0.5)\n",
    "    html = browser.find_element_by_xpath(\"//*\").get_attribute('outerHTML')\n",
    "    selector = Selector(text=html)\n",
    "    \n",
    "    table = selector.css('table.comment_table')\n",
    "    if len(table) == 2:\n",
    "        rows = table[1].xpath('.//tr[contains(@id, \"ct_\")]')\n",
    "    elif len(table) == 1:\n",
    "        rows = table[0].xpath('.//tr[contains(@id, \"ct_\")]')\n",
    "    datetimes = []\n",
    "    nicks = []\n",
    "    comments = []\n",
    "    likes = []\n",
    "    dislikes = []\n",
    "    for row in rows:\n",
    "        nick = row.css('div.nick').css('strong').xpath('./text()')[0].extract().encode('utf-8')\n",
    "        try:\n",
    "            comment = row.css('div.text_wrapper').xpath('./span[@class=\"text\"]/text()')[0].extract().encode('utf-8')\n",
    "            comment = re.sub(r'[\\n+\\s+]',' ',comment)\n",
    "        except:\n",
    "            print('removedComment')\n",
    "            comment = \"\"\n",
    "        datetime = row.css('span.time').xpath('./text()')[0].extract().encode('utf-8')\n",
    "        like = row.css('button.btn_like').xpath('./span/text()')[0].extract().encode('utf-8')\n",
    "        dislike = row.css('button.btn_dislike').xpath('./span/text()')[0].extract().encode('utf-8')\n",
    "        datetimes.append(datetime)\n",
    "        nicks.append(nick)\n",
    "        comments.append(comment)\n",
    "        likes.append(like)\n",
    "        dislikes.append(dislike)\n",
    "    t2 = time.time()\n",
    "    print(str(t2-t1))\n",
    "    return [datetimes,nicks,comments,likes,dislikes]\n",
    "\n",
    "def getCommentInfo(url):\n",
    "    browser.get(url)\n",
    "    try:\n",
    "        Page_comment = getCommentInfo_Page()\n",
    "    except:\n",
    "        df = pd.DataFrame({\"datetime\":([\"9999.9.9\"]),\"nicks\":[\"nodata\"],\"comment\":[\"nodata\"],\"like\":[0],\"dislike\":[0]},\n",
    "                          columns=[\"datetime\",\"nicks\",\"comment\",\"like\",\"dislike\"])\n",
    "        print(\"no comment\")\n",
    "        return df\n",
    "\n",
    "    while True:\n",
    "        paging = browser.find_element_by_css_selector(\"div.paging_wrapper.row.bottom\")\n",
    "        pagebtn = paging.find_elements_by_class_name(\"btn_num\")\n",
    "        if len(pagebtn) == 1:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "        for i in range(1,len(pagebtn)):\n",
    "            pagebtn[i].click()\n",
    "            Page_comment_next =  getCommentInfo_Page()\n",
    "            Page_comment = [i+j for i,j in zip(Page_comment,Page_comment_next)]\n",
    "            paging = browser.find_element_by_css_selector(\"div.paging_wrapper.row.bottom\")\n",
    "            pagebtn = paging.find_elements_by_class_name(\"btn_num\")\n",
    "        if len(pagebtn) != 10:\n",
    "            break\n",
    "        try:\n",
    "            paging.find_element_by_class_name('btn_next').click()\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            break\n",
    "    datetime,nicks,comment,like,dislike = Page_comment[0],Page_comment[1],Page_comment[2],Page_comment[3],Page_comment[4]\n",
    "    df = pd.DataFrame({\"datetime\":datetime,\"nicks\":nicks,\"comment\":comment,\"like\":like,\"dislike\":dislike},columns=[\"datetime\",\"nicks\",\"comment\",\"like\",\"dislike\"])\n",
    "    return df\n",
    "\n",
    "def getTargetInfo():\n",
    "    info = pd.read_excel(\"C:\\\\Users\\\\Yunwoo\\\\Desktop\\\\rurliweb_crawl-master\\\\target_info.xlsx\")\n",
    "    return info.P_KEY\n",
    "\n",
    "def getTargetUrl():\n",
    "    url = pd.read_excel(\"C:\\\\Users\\\\Yunwoo\\\\Desktop\\\\rurliweb_crawl-master\\\\target_url.xlsx\")\n",
    "    return url.urls\n",
    "\n",
    "def main():\n",
    "    urlList = getTargetUrl()\n",
    "    p_keys = getTargetInfo()\n",
    "    df = getCommentInfo(urlList[0])\n",
    "    pn = pd.DataFrame({\"POSTNUM\":[p_keys[0] for _ in range(len(df))]})\n",
    "    df = pd.concat([df,pn],axis=1)\n",
    "    checker = 0\n",
    "    for url,pkey in zip(urlList[1:],p_keys[1:]):\n",
    "        df_i = getCommentInfo(url)\n",
    "        pn_i = pd.DataFrame({\"POSTNUM\":[pkey for _ in range(len(df_i))]})\n",
    "        df_i = pd.concat([df_i,pn_i],axis=1)\n",
    "        df = pd.concat([df,df_i],axis = 0)\n",
    "        print(checker)\n",
    "        checker += 1\n",
    "    df.to_csv(\"C:\\\\Users\\\\Yunwoo\\\\Desktop\\\\commentData.csv\")\n",
    "    return 0\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.552000045776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>nicks</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>dislike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.02.04 16:00</td>\n",
       "      <td>화가난 플벌레</td>\n",
       "      <td>나는 전설이다.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          datetime    nicks   comment like dislike\n",
       "0  12.02.04 16:00   화가난 플벌레  나는 전설이다.    0       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCommentInfo('http://bbs.ruliweb.com/news/board/11/read/121?page=48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
